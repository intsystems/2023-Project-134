\documentclass[12pt]{article}
\usepackage{arxiv}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage[unicode, pdftex]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{algpseudocode}
\usepackage[margin=3cm]{geometry}
\usepackage{algorithm2e}
\usepackage{ dsfont }

%\documentclass{article}
\usepackage{array}
\usepackage{multirow}


\usepackage{setspace}
\singlespacing % полуторный интервал для всего текста


\title{Дистилляция моделей и данных}

\author{ Баринов Никита\\
	МФТИ\\
	\And
	Филатов Андрей \\
	МФТИ       
}
\date{}

\renewcommand{\undertitle}{}
\renewcommand{\headeright}{}
\renewcommand{\shorttitle}{Дистилляция моделей и данных}

\hypersetup{
pdftitle={Дистилляция моделей и данных},
pdfauthor={Баринов Никита},
pdfkeywords={Deep Learning \and Distilling the Knowledge \and Dataset Distillation}
}

\begin{document}
\maketitle

\begin{abstract}

%отдельно про дистилляцию моделей - сократить размер модели
%данных - обучить две модели можем
% не сущ-т решения одновременной 
% провели рез-ты и получили качество не сильно хуже
Во многих задачах машинного обучения точность предсказания модели зависит от её размера. При этом, чтобы получить хорошее качество, требуется много данных, что в свою очередь увеличивает время обучения. Дистилляция моделей позволяет уменьшить их размер, при этом не сильно потеряв в качестве, а дистилляция данных позволяет существенно снизить время обучения. В статье мы предлагаем использовать модель-эксперт, которая дистиллируется. Это позволяет ускорить сходимость моделей при дистилляции данных и качество моделей, обученных на этих данных. Такой процесс называется дистилляцией моделей и данных, и в статье мы предлагаем одно из решений, а для сравнения и обработки результатов провели эксперименты на выборке CIFAR10





    % Во многих задачах ML точность предсказания модели зависит от её размера. При этом зачастую данная зависимость выглядит достаточно тривиально: последовательное увеличение размеров модели позволяет последовательно улучшать точность её предсказаний. Но такой безграничных рост приводит к ряду проблем: существенное увеличение времени обучения, высокие требования к размерам и качеству обучающей выборки, а также вычислительные сложности. Аналогичная проблема и с данными: чем их больше, тем дольше модель на них обучается.
    % В связи с этим возникает желание одновременно <<сжать>> и данные, и модели так, чтобы на новых данных модель меньшего размера не сильно теряла в качестве. Этот процесс называется дистилляцией моделей и данных, и в статье мы предлагаем одно из решений. Вычислительные эксперименты проводятся на выборке изображений CIFAR10. 


\end{abstract}

\keywords{Deep Learning \and Distilling the Knowledge \and Dataset Distillation \and Model Compression}


\section{Introduction}

Глубокое обучение добилось огромного успеха за последние несколько лет в различных областях, таких как компьютерное зрение(\cite{guo2022attention}), обработка естественного языка(\cite{raina2022natural}) и распознавание речи(\cite{subramanian2022deep}). Но всё это требует больших вычислительные и временные ресурсы. Например, как говорится в оригинальной статье \cite{radford2019language}, GPT-2 с 1,5 миллиардом параметров было использовано 40 терабайт текстовых данных, и модель обучалась на суперкомпьютерных кластерах в течение нескольких недель.

Со временем начали появляться методы <<сжатия>> моделей без сильной потери качества. Появилась концепция дистилляции знаний(knowledge distillation) – это способ обучения нейросетевых моделей машинного обучения, направленный на передачу знаний от модели-учителя к модели-ученику. В широком диапазоне практически значимых задач машинного обучения точность предсказания модели существенно зависит от её размера. При этом зачастую данная зависимость выглядит очень просто: последовательное увеличение размеров модели позволяет последовательно улучшать точность её предсказаний. Однако такой безграничный рост приводит к ряду проблем, связанных с практическим применением итоговых моделей. Первой статьёй, в которой можно встретить дистилляцию знаний в современном виде является \cite{hinton2015distilling}. В ней предлагается сжать ансамбль моделей в одну модель, тем самым значительно уменьшив её размер.

Поскольку проблема есть и с большим объёмом денных, появляется дистилляция данных(dataset distillation) - это существенное уменьшение выборки, путём создания искусственных объектов (синтетических данных), которые агрегируют полезную информацию, хранящуюся в данных, и позволяют настраивать алгоритмы машинного обучения не менее эффективно, чем на всех данных. Если мы имеем лишь несколько достаточно хорошо дистиллированных изображений, мы можем гораздо эффективнее обучить нейронную сеть на целом наборе данных, по сравнению с традиционным обучением, при котором часто используются десятки тысяч шагов градиентного спуска. Каждый элемент синтетических данных содержит в себе больше информации, чем отдельный элемент исходной выборки. 

В данной работе предлагается новый подход: одновременная дистилляция модели и данных методами \cite{hinton2015distilling} и \cite{cazenavette2022dataset}. Выборку CIFAR10 мы сократили до десяти изображений на каждый из десяти классов, в качестве модели-учителя выступила ResNet50, а в качестве модели-ученика - ConvNet. Мы привели несколько методов одновременной дистилляции и сравнили их эффективность на тестовой части выборки CIFAR10. 

\section{Related works}

 Сегодня существует несколько решений проблемы дистилляции моделей или данных в отдельности. 

\subsection{Дистилляция моделей}
%применили дистилляцию чтобы сжать модель ансамбль в одну модель
В статье \cite{romero2014fitnets} говорится, что промежуточные веса или особенности слоев могут также использоваться для обучения меньшей модели. Ещё один способ дистилляции, описанный в статье \cite{ba2014deep}, он основан на том, что меньшая модель(ученик) обучается аналогично большей(учитель), тем самым получается конкурентоспособная производительность. В \cite{hinton2015distilling} применили дистилляцию, чтобы сжать ансамбль в одну модель. Одной из последних работ является \cite{chung2020feature}. В ней описывается дистилляция в онлайн-режиме: модель и ученик совместно оптимизируются на каждой итерации. Также существует кросс-модельная дистилляция(передача знаний между промежуточными моделями), одним из сценариев котороя является \cite{chen2021learning}: имеется граф взаимоотношений между моделями, а передача знаний осуществляется при помощи предложенной функции потерь, сохраняющей локальность.

%найти поновее статьи, страница 11, multi-teacher дистилл.

\subsection{Дистилляция данных}
%порядок статей временной
В статье \cite{wang2018dataset} сначала данные инициализируются случайным шумом, а затем при помощи градиентного спуска происходит обновление синтетических данных. Описанный метод имеет явный недостаток: он ограничен числом эпох обучения. Использование теоремы о неявной функции в \cite{lorraine2020optimizing} помогает избавиться от такого недостатка. В \cite{zhao2020dataset} в качестве функции ошибки используется расстояние между градиентами этой ошибки по параметрам ученика, которые получаются при обучении на обычных и дистиллированных данных. Альтернативным вариантом может быть введение генеративной модели(может создавать новые данные, которые похожи на те, что были использованы для ее обучения), способной из шума и меток класса создавать необходимые для обучения синтетические изображения, этот подход подробно описан в \cite{such2020generative}.
Статья \cite{cazenavette2022dataset} предлагает метод дистилляции путем создания выборки, на которой динамика обучения такая же, как и на исходной.
%почему теорема о неявной функции полезна
%ванг ограничен числом эпох, а эти ограничения обошлись применением теоремы о неявной функции
%жао в кач-ве ф.ош.  исп расст....


%абстрактная вещь в начале дип лернинг чето там
%подвод к дистилляции данных, в чем прикол, позволит обечать модели намного быстрее и тд
%во многих задачах модели сильно избыточные, и для того чтобы сохранить кач-во и обобщ спос большой модели применяется подходит дистилляции моделей
% в чем вклад: предложили то то то
%в методее рассм применение, как можно примениь на релаьных задачах



\section{Постановка проблемы}

%два аргмина: модель: лосс

В этом разделе описывается формальная постановка проблем дистилляции данных, дистилляции моделей и предлагаемое решениеодновременной дистилляции моделей и данных.

\subsection{Дистилляция данных}

Пусть $\mathcal{D}_{real} = \{(\mathbf{x_i}, y_i)\}_{i = 1}^N$ --- исходная выборка. Наша задача --- создать меньшую выборку $\mathcal{D}_{syn} = \{(\mathbf{\hat x_i}, \hat y_i\}_{i = 1}^M$, где $M \ll N$ и такой, что качество модели, обученной на нём сопоставимо с качеством при обучении на исходных данных. Наш метод дистилляции предполагает создание экспертных траекторий обучения $\tau^*$, под которыми понимается последовательность параметров $\{ \theta_t^*\}_{t = 0}^T$, полученных во время обучения нейронной сети на $\mathcal{D}_{real}$. Чтобы получить экспертные траектории, предлагается обучить большое количество нейронных сетей на $\mathcal{D}_{real}$ и сохраним их параметры на каждой эпохе. Также определим $\hat\theta_t$ - параметры модели-студента, обученной на $\mathcal{D}_{syn}$ на шаге обучения $t$. На каждом шаге обучения мы будем выбирать случайно $\theta_t^*$, инициализировать этим значением парметры модели-студента $\theta^*_t := \theta_t^*$. Установим верхнюю границу $T^{max}$ на число $t$, чтобы игнорировать ту часть обучения, где параметры меняются незначительно. 

Пусть $l(\mathcal{A}(\mathcal{D}_{syn}), \theta_t)$ - дифференцируемая функция потерь, $\mathcal{A}$ - дифференцируемая техника аугментации данных(это метод, используемый в машинном обучении, для увеличения размера обучающего набора данных путем создания новых примеров на основе существующих данных.) \cite{romero2014fitnets}. После инициализации параметров модели-студента мы совершим $N$ шагов градиентного спуска по параметрам $\hat\theta_t$:
\[
\hat\theta_{t+n+1} = \hat\theta_{t+n} - \alpha\nabla l(\mathcal{A}(\mathcal{D}_{syn}), \hat\theta_{t+n}),
\eqno(1)
\]

где $\alpha$ - шаг обучения модели-студента, используемый для обновления её параметров. 
После обучения градиентного спуска для конкретной траектории $\tau^* \in \{\tau_i^*\}$ считаем 
\[
\mathcal{L} = \frac{\parallel\hat\theta_{t+N}-\theta_{t+M}^*\parallel_2^2}{\parallel\theta_t^* - \theta_{t+M}^* \parallel_2^2},
\eqno(2)
\]

где $\mathcal{L}$ - функция потерь между конечными параметрами студента и учителя, нормированная на пройденное учителем расстояние, что помогает получать информацию о более поздних стадиях его обучения, где параметры меняются не сильно. В конце мы обновляем $\mathcal{D}_{syn}$ в соответствии с обучаемым параметром $\alpha$ и посчитанной функцией $\mathcal{L}$. Итоговый алгоритм выглядит так:

\RestyleAlgo{ruled}
%% This is needed if you want to add comments in
%% your algorithm with \Comment
\SetKwComment{Comment}{/* }{ */}
\begin{algorithm}[hbt!]
\caption{Data Distillation}\label{alg:two}
\KwData{\{\tau_i^*\} - \text{множество параметров учителей, обученных на }\mathcal{D}_{real}}
\KwData{$M$ - \text{число обновлений между стартовыми и целевыми параметрами учителя}}
\KwData{$N$ - \text{число обновлений студента за один шаг дистилляции}}
\KwData{$\mathcal{A} - \text{дифференцируемая функция аугментации}$}
\KwData{$T^{max} < T - \text{максимальная стартовая эпоха}$}
\KwResult{\text{Дистиллированный набор } $\mathcal{D}_{syn}$ и $\alpha$}
$\mathcal{D}_{syn} \gets \mathcal{D}_{real}$\;
$\alpha \gets \alpha_0$\;
\For{$step : 1 \ .. \ N$}{
  $\tau^* \sim \{\tau_i^*\}, \tau^* = \{\theta_t^*\}_0^T$ - \text{выбираем траекторию обучения}\;
  $t \leq T^*$ - \text{случайно выбираем начальную эпоху}\;
  $\theta^*_t := \theta_t^*$ - \text{инициализируем веса студента параметрами учителя}\;
  \For{$n : 0 \ .. \ N-1$}{
    $b_{t+n} \sim \mathcal{D}_{syn}$ - \text{выбрать мини-батч из }$\mathcal{D}_{syn}$\;
    $\hat\theta_{t+n+1} \gets \hat\theta_{t+n} - \alpha\nabla l(\mathcal{A}(\mathcal{D}_{syn}), \hat\theta_{t+n})$\;
  }
  $\mathcal{L} \gets \parallel\hat\theta_{t+N}-\theta_{t+M}^*\parallel_2^2 / \parallel\theta_t^* - \theta_{t+M}^* \parallel_2^2$\;
  \text{Изменить } $\mathcal{D}_{syn}$ \text{ и } $\alpha$ \text{ в зависимости от } $\mathcal{L}$\;
  }
\end{algorithm}


Итого первая оптимизационная задача, которая решает дистилляцию данных, выглядит так:
\[
\mathbf{\hat X}, \alpha = \underset{\mathbf{x}, \alpha}{\arg\min}~ \mathcal{L}(\mathbf{x}, \alpha, \theta), \ \mathcal{D}_{syn} = \underset{\mathbf{\hat x} \in \mathbf{\hat X} }{\bigcup}~\mathbf{\hat x}.
\eqno(3)
\]

\subsection{Дистилляция моделей}

Далее стоит задача обучения нейросети на дистиллированных данных $\mathcal{D}_{syn}$ и дистилляция модели. 

$\mathbf{Def \ 1:}$ Дистилляция модели - снижение сложности модели путем выбора модели в множестве более простых моделей на основе анализа пространства параметров и предсказаний целевой перменной более сложной фиксированной модели.

$\mathbf{Def \ 2:}$ Учитель - фиксированная модель, ответы которй используются при выборе модели-ученика.

$\mathbf{Def \ 3:}$ Ученик - модель, которая выбираемся согласно заданному критерию качества учителя.

Итак, решается задача класификации:
\[
\mathcal{D} = \{ (\mathbf{\hat x_i}, y_i) \}_{i=1}^R,
\]
где $y_i \in \mathds{Y} = {1,2,...,R}$, $R$ - число классов, $\mathbf{\hat x_i} \in \mathds{R}^n$.

В дистилляции Хинтона \cite{hinton2015distilling} рассматривается параметрическое семейство функций:
\[
\mathcal{G} = \{\mathbf{g} \ | \ \mathbf{g} = softmax(\mathbf{z}(\mathbf{x})/T), \ \mathbf{z}: \mathds{R}^n \rightarrow{} \mathds{R}^R\},
\eqno(4)
\]
где $\mathbf{z}$ - дифференцируемая параметрическая функция заданной структуры, $T$ - параметр температуры. В качестве модели-учителя рассматривается функция $\mathbf{f}$ из множества:
\[
\mathcal{F} = \{\mathbf{f} \ | \ \mathbf{f} = softmax(\mathbf{v}(\mathbf{x})/T, \ \mathbf{v}: \mathds{R}^n \rightarrow{} \mathds{R}^R  \},
\eqno(5)
\]
где $\mathbf{z}$ - дифференцируемая параметрическая функция заданной структуры, $T$ - параметр температуры. 

При этом температура $T$ имеет свойства:

\begin{enumerate} 
\item при Т $\rightarrow 0$ получаем вектор, в котором один из классов имеет единичную вероятность;
\item при Т $\rightarrow \infty$ получаем вектор, в котором все классы равновероятны.
\end{enumerate} 

Функция потерь $\mathcal{L}$ учитывает перенос инфорации от модели-учителя $\mathbf{f}$ к ученику $\mathbf{g}$ и имеет вид:

\[
\mathcal{L}(\mathbf{g}) = - \sum\limits_{i=1}^m \sum\limits_{r=1}^R y_i^r \log\mathbf{g}(\mathbf{x_i}) \bigg|_{T=1} - \sum\limits_{i=1}^m \sum\limits_{r=1}^R \mathbf{f}(\mathbf{x_i}) \bigg|_{T=T_0} \log\mathbf{g}(\mathbf{x_i}) \bigg|_{T=T_0},
\eqno(6)
\]

где первое слаагемое отвечает за исходную функцию потерь, а второе - за дистилляцию. Итого получаем оптимизационную задачу:
\[
\mathbf{\hat g} = \underset{\mathbf{g} \in \mathcal{G}}{\arg\min}~\mathcal{L}(\mathbf{g}).
\eqno(7)
\]

\subsection{Дистилляция моделей и данных}

В нашей работе мы предлагаем модифицированный способ дистилляции: в функцию потерь, которая используется для дистилляции данных, добавим слагаемое, отвечающее за дистилляцию знаний от модели-учителя. Тем самым, новая функция потерь будет иметь вид: 




% дистилляция моделей, acc на тесте mnist, описать параметры все эпохи, опт ...
% resnet50-teacher, resnet18-student. pretrained=False. torchvision.models.resnet18
% поменять число каналов на 1
% итог - две цифры два аккураси
% логгер: импорт самари райтер и во время обучения записывать в него
% таблица(-цы)
%

\section{Эксперимент}

\subsection{Базовый эксперимент}

Проведем базовый эксперимент на выборке CIFAR10, с которым будем сравнивать результаты в дальнейшем: дистилялция моделей. Для этого в качестве модели-учител используем ResNet50, в качестве модели-ученика ConvNet. 

\begin{table}[htbp]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    Модель & Accuracy \\ \hline
    ConvNet & ---------\% \\ \hline
    ResNet50 & 74.83\% \\ \hline
  \end{tabular}
  \caption{Точность моделей в отдельности}
  \label{tab:my-table}
\end{table}

Зависимоcть функции потерь и точности от эпохи обучения для каждой модели в отдельности можно видеть на графиках:

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{resnet50lrcurve.jpg}
\caption{Процесс обучения ResNet50}
\label{fig:имя_файла}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{resnet18lrcurve.jpg}
\caption{Процесс обучения ConvNet}
\label{fig:имя_файла}
\end{figure}

\newpage
Теперь используем дистилляцию моделей. Тут вводится понятие distillation weight - коэффициент влияния модели-ученика. Обозначим его $\alpha$, тогда функция потерь ученика примет вид:
$$(1 - \alpha) \cdot H(p, q) + \alpha \cdot D_{KL}(p|q) ,$$ где кросс-энтропия вычисляется по формуле:
$$ H(p,q) = -\sum_{x} p(x) \log q(x) ,$$
а дивергенция Кульбака-Лейблера:
$$ D_{KL}(p|q) = \sum_{x} p(x) \log \frac{p(x)}{q(x)} .$$
Мы использовали оптимизатор $SGD$ и 6 эпох градиентного спуска.
Результаты представлены в таблице:

\begin{table}[htbp]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    \alpha & Функция потерь & Accuracy \\ \hline
    0.25 & $0.75\cdot H(p,q)+0.25\cdot D_{KL}(p|q)$ & 73.30\% \\ \hline
    0.5 & $0.5\cdot H(p,q)+0.5\cdot D_{KL}(p|q)$ & 71.71\% \\ \hline
    0.75 & $0.25\cdot H(p,q)+0.75\cdot D_{KL}(p|q)$ & 69.43\% \\ \hline
  \end{tabular}
  \caption{Дистилляция моделей}
  \label{tab:my-table}
\end{table}

\mathbf{Вывод:} дистилляция моделей не сильно ухудшила качество, причем качество немного падает с уменьшением влияния учителя.



\subsection{Основной эксперимент}

ResNet50, обученный на всех данных, добавляется в лосс дистилляции данных. 


Мы получили дистиллированные изображения для $T=0.5, 1, 3$ и $\alpha = 0.3, 0.7$

Итого, функция потерь, по которой происходит дистилляция моделей и данных:

$$
l() = (1 - \alpha) \cdot H(p, q) + \alpha \cdot D_{KL}(p|q)
$$

\begin{table}[htbp]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    T & Accuracy \\ \hline
    0.25 & 73.30\% \\ \hline
    0.5 & 71.71\% \\ \hline
    0.75 & 69.43\% \\ \hline
  \end{tabular}
  \caption{Дистилляция моделей и данных, $\alpha = 0.3$}
  \label{tab:my-table}
\end{table}

\begin{tabular}{|c|>{\centering\arraybackslash}m{4cm}|}
  \hline
  Первый столбец & Второй столбец \\
  \hline
  \multirow{2}{*}{Строка 1} & Строка 1, часть 1 \\
                            & Строка 1, часть 2 \\
  \hline
  \multirow{2}{*}{Строка 2} & Строка 2, часть 1 \\
                            & Строка 2, часть 2 \\
  \hline
\end{tabular}




% табличка с обучением на дистиллированном датасете
Обучаем модель ученика:

$Loss = (1 - \alpha) * CE + \alpha * KLloss$

% мотивация 

% убрать число эпох, написать оптимизатор

% Базовый эксперимент
%  ResNet50 учим на дистилированных данных, дистиллируем в ConvNet на дистилированных данных
%  ResNet50 учим на всех данных, дистиллируем в ConvNet на дистилированных данных
%  ResNet50 на дистилированных данных
%  ConvNet на дистилированных данных
%  ConvNet на всех данных
%  ResNet50 на всех данных

% Основной эксперимент
% ResNet50 обученный на всех данных добавляется в лосс дистилляции данных. Учим ConvNet на супердистилированных данных.
% Учим ResNet50 на супердистилированных данных.
 
% СуперТаблица


% На будущее график по данным 
% интерес узнать зависимость качества от числа данных
% идеальный баланс для сжатия моделей и данных
% кажется, что когда меньше картинок на класс эффект дистилляции моделей должен быть больше


\section{Вывод}

% начать писать concl что пока видим


\newpage
\bibliographystyle{unsrt}
\bibliography{ref}

\end{document}